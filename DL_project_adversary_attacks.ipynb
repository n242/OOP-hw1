{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_project-adversary_attacks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxqC96Y/1kwgKwJnkGlsBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n242/OOP-hw1/blob/master/DL_project_adversary_attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9XA7jz320MgQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import NoneType\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "from torchsummary import summary\n",
        "\n",
        "# for adversary\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    transform_train = transforms.Compose([\n",
        "        # agmentation below\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # regular normalization\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    ])\n",
        "\n",
        "    # Normalize the test set same as training set without augmentation\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    print(f' trainset: {trainset}')\n",
        "    # print(f' trainset shape: {trainset.size()}')\n",
        "\n",
        "    ## script to find mean\n",
        "    # data = trainset.data / 255  # data is numpy array\n",
        "    #\n",
        "    # mean = data.mean(axis=(0, 1, 2))\n",
        "    # std = data.std(axis=(0, 1, 2))\n",
        "    # print(f\"Mean : {mean}   STD: {std}\")  # Mean : [0.491 0.482 0.446]   STD: [0.247 0.243 0.261]\n",
        "\n",
        "    cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                     transform=transform_test)  # transform_test\n",
        "    cifar_trainset, cifar_valset = data.random_split(trainset, [int(len(trainset) * 0.8), int(len(\n",
        "        trainset) * 0.2)])  # split the trainset to trainset and validation set in 80%-20% retio\n",
        "\n",
        "    print('train set len', len(cifar_trainset))\n",
        "    print('validation set len', len(cifar_valset))\n",
        "    print('test set len', len(cifar_testset))\n",
        "\n",
        "    number_workers = 0\n",
        "    if device == torch.device('cuda'):\n",
        "        number_workers = 2\n",
        "    train_loader = data.DataLoader(cifar_trainset, shuffle=True, batch_size=64, num_workers=number_workers)\n",
        "    val_loader = data.DataLoader(cifar_valset, shuffle=False, batch_size=64, num_workers=number_workers)\n",
        "    test_loader = data.DataLoader(cifar_testset, shuffle=False, batch_size=64, num_workers=number_workers)\n",
        "\n",
        "    test_for_adv = data.DataLoader(cifar_testset, shuffle=False, batch_size=1)\n",
        "    train_for_adv = data.DataLoader(cifar_trainset, shuffle=True, batch_size=1)\n",
        "    return train_loader, val_loader, test_loader, test_for_adv, train_for_adv\n",
        "\n",
        "\n",
        "# model3:  with dropout, with batch, without fc layers\n",
        "\n",
        "\n",
        "class CNN_model(nn.Module):  # TODO: fix so I get correct dimensions of output\n",
        "    def __init__(self):\n",
        "        super(CNN_model, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=64, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=1),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv layers\n",
        "        features = self.feature_extractor(x)\n",
        "        # print(\"features shape:\", features.shape)\n",
        "\n",
        "        # final non fully connected\n",
        "        class_scores = self.classifier(features)\n",
        "        # print(\"class_scores shape:\", class_scores.shape)\n",
        "        class_scores = torch.reshape(class_scores, (class_scores.size(dim=0), class_scores.size(dim=1)))\n",
        "        # print(\"class_scores shape:\", class_scores.shape)\n",
        "\n",
        "        return class_scores\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_data(model, epochs, learning_rate, loss_function, train_loader, valid_loader, patience=4):\n",
        "    loss_arr = []\n",
        "    avg_train_loss_arr, avg_val_loss_arr = [], []\n",
        "    train_acc_arr, val_acc_arr = [], []\n",
        "    # Early stopping  parameters\n",
        "    last_loss = 100  # initializing max loss as high unreachable value\n",
        "    trigger_times = 0\n",
        "    PATH = './checkpoint'\n",
        "    total, correct = 0.0, 0.0\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00012)\n",
        "\n",
        "    dataiter = iter(train_loader)\n",
        "    images, labels = dataiter.next()\n",
        "    print(type(images))\n",
        "    print(images.shape)\n",
        "    print(labels.shape)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # defining we're training so can use dropout, batch norm\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            if device == torch.device('cuda'):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward propagation\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Show progress\n",
        "            if i % 100 == 0 or i == len(train_loader):\n",
        "                # print('[{}/{}, {}/{}] loss: {:.8}'.format(epoch, epochs, i, len(train_loader), loss.item()))\n",
        "                print(\"Iteration: {0} | Loss: {1} | index {2} \".format(epoch, loss.item(), i))\n",
        "\n",
        "            total += inputs.shape[0]\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "        # print(\"total is: {0}, len(train_loader): {1}, correct pred num is: {2}\".format(total, len(train_loader), correct))\n",
        "        train_acc = (correct / total)\n",
        "        print('Accuracy: in train', train_acc)\n",
        "        train_acc_arr.append(train_acc.item())\n",
        "\n",
        "        plot_graph(loss_arr, \"generic network training loss\")\n",
        "        avg_train_loss_arr.append(np.mean(loss_arr))\n",
        "        # Early stopping\n",
        "        current_loss, avg_val_loss, val_acc = validation_data(model, valid_loader)\n",
        "        print('The Current Loss by validation data:', current_loss)\n",
        "        avg_val_loss_arr.append(avg_val_loss.item())\n",
        "        val_acc_arr.append(val_acc.item())\n",
        "\n",
        "        if current_loss > last_loss:\n",
        "            trigger_times += 1\n",
        "            # print('Trigger Times:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break  # exit loop, print data\n",
        "\n",
        "        else:\n",
        "            # print('trigger times did not increase:' , trigger_times)\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            trigger_times = 0\n",
        "\n",
        "        last_loss = current_loss\n",
        "\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    plot_graph(loss_arr, \"generic network training loss\")\n",
        "\n",
        "    plt.plot(avg_train_loss_arr, label='train loss')\n",
        "    # Plot another line on the same chart/graph\n",
        "    plt.plot(avg_val_loss_arr, label='val loss')\n",
        "    plt.title(\"avg train loss vs avg validation loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    # plt.savefig('./outputs_q1/avg_train_loss_vs_avg_validation_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(train_acc_arr, label='train accuracy')\n",
        "    # Plot another line on the same chart/graph\n",
        "    plt.plot(val_acc_arr, label='validation accuracy')\n",
        "    plt.title(\"avg train acc vs avg validation acc\")\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    # plt.savefig('./outputs_q1/avg_train_acc_vs_avg_validation_acc.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_graph(list, title):\n",
        "    plt.plot(list)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def validation_data(model, valid_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss_total = 0.0\n",
        "    loss_arr = []\n",
        "\n",
        "    # iterate over test data\n",
        "    with torch.no_grad():  # disable gradients because we only run on test data\n",
        "        for (data, labels) in valid_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device == torch.device('cuda'):\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss_valid = loss_fn(output, labels)\n",
        "            loss_arr.append(loss_valid.item())\n",
        "            loss_total += loss_valid.item()\n",
        "\n",
        "            total += data.shape[0]\n",
        "            predictions = torch.argmax(output.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "    # plot_graph(loss_arr, \"generic network valid loss\")\n",
        "    acc = (correct / total)\n",
        "    print('Accuracy: in validation', acc)\n",
        "\n",
        "    return (loss_total / len(valid_loader)), np.mean(loss_arr), acc\n",
        "\n",
        "\n",
        "def test_data(model, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss_arr = []\n",
        "\n",
        "    # iterate over test data\n",
        "    with torch.no_grad():  # disable gradients because we only run on test data\n",
        "        for (data, labels) in test_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device == torch.device('cuda'):\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss_test = loss_fn(output, labels)\n",
        "            loss_arr.append(loss_test.item())\n",
        "\n",
        "            total += data.shape[0]\n",
        "            predictions = torch.argmax(output.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "    plot_graph(loss_arr, \"generic network test loss\")\n",
        "\n",
        "    print('Accuracy: in test', (correct / total))\n",
        "\n",
        "\n",
        "def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10): # overshoot prevents vanishing updates\n",
        "    model.eval()\n",
        "    #print('in deepfool before first forward')\n",
        "\n",
        "    image.requires_grad = True\n",
        "    out_orig = model.forward(image)\n",
        "\n",
        "    temp = np.array(out_orig.cpu().detach().numpy()) #getting image prediction labels\n",
        "    labels = temp.flatten().argsort()[::-1]  # labels indexes from low to high\n",
        "    #print(labels)\n",
        "    #print('after first')\n",
        "\n",
        "    labels = labels[0:num_classes] #starting with label with highest prediction\n",
        "    label = labels[0]\n",
        "\n",
        "    input_shape = image.detach().shape\n",
        "    pert_image = copy.deepcopy(image)\n",
        "    w = np.zeros(input_shape)\n",
        "    pert_tot = np.zeros(input_shape)\n",
        "    ctr = 0\n",
        "\n",
        "    x = pert_image[None, :].clone().detach().requires_grad_(True).to(device)\n",
        "    x.retain_grad()\n",
        "    out = model.forward(x[0])\n",
        "    #print('after second forward')\n",
        "    label_cur = label\n",
        "\n",
        "    while label_cur == label and ctr < max_iter:  # and x.grad is not None\n",
        "\n",
        "        pert = np.inf\n",
        "        #print('before backward 1')\n",
        "        out[0, labels[0]].backward(retain_graph=True) # retain_graph for iterating through the graph after the first time\n",
        "        #print(x.grad)\n",
        "        grad_orig = x.grad.data.cpu().detach().numpy().copy()\n",
        "\n",
        "        for i in range(1, num_classes): #finding hyperplane which gives smallest difference between all classes\n",
        "            #print('before backward 2')\n",
        "            out[0, labels[i]].backward(retain_graph=True)\n",
        "            cur_grad = x.grad.data.cpu().detach().numpy().copy()\n",
        "\n",
        "            w_cur = cur_grad - grad_orig\n",
        "            out_diff = (out[0, labels[i]] - out[0, labels[0]]).data.cpu().detach().numpy() # difference between prediction of original image and perturbed\n",
        "\n",
        "            # using formula to calculate current hyperplane\n",
        "            hyperplane_cur = abs(out_diff) / np.linalg.norm(w_cur.flatten())\n",
        "\n",
        "            # getting minimal change hyperplane\n",
        "            if hyperplane_cur < pert:\n",
        "                pert = hyperplane_cur\n",
        "                w = w_cur\n",
        "\n",
        "        # Added 1e-4 for numerical stability\n",
        "        pert_cur = (pert + 1e-4) * w / np.linalg.norm(w)\n",
        "        # calculating new perturbed image to updated image under min hyperplane such that her projection changed\n",
        "        pert_tot = np.float32(pert_tot + pert_cur)\n",
        "\n",
        "        pert_image = image.cpu().detach() + (1 + overshoot) * torch.from_numpy(pert_tot)\n",
        "\n",
        "        x = pert_image.clone().detach().requires_grad_(True).to(device)\n",
        "        x.retain_grad()\n",
        "        #print('before final forward')\n",
        "        out = model.forward(x[0])\n",
        "        label_cur = np.argmax(out.data.cpu().detach().numpy().flatten())  # label of pert image\n",
        "\n",
        "        ctr += 1\n",
        "\n",
        "    pert_tot = (1 + overshoot) * pert_tot\n",
        "\n",
        "    return pert_tot, ctr, label, label_cur, pert_image\n",
        "\n",
        "\n",
        "def batched_deepfool(model, batch):\n",
        "    sum_diff = 0\n",
        "    adv_images = []\n",
        "    for j in range(len(batch)):\n",
        "        batch[j].requires_grad = True\n",
        "        r, loop_i, label_orig, label_pert, pert_image = deepfool(batch[j].unsqueeze(0), model, max_iter=50)\n",
        "        if label_orig != label_pert:\n",
        "            sum_diff += 1\n",
        "        adv_images.append(pert_image.detach().squeeze().to(device))\n",
        "    # print(pert_image.detach().squeeze().to(device).shape)\n",
        "    # print(torch.stack(adv_images).shape)\n",
        "    final = torch.stack(adv_images)\n",
        "    return final\n",
        "\n",
        "\n",
        "def calling_deepfool(model, xLoader):\n",
        "\n",
        "    sum_diff = 0\n",
        "    for i, (image, label) in enumerate(xLoader):\n",
        "        if device == torch.device('cuda'):\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "        #image.requires_grad = True\n",
        "        r, loop_i, label_orig, label_pert, pert_image = deepfool(image, model, max_iter=50)\n",
        "        if label_orig != label_pert:\n",
        "            sum_diff+=1\n",
        "        # if sum_diff ==10:\n",
        "        #     print(f'index is: {i}, sum_diff is: {sum_diff}')\n",
        "        #     break\n",
        "    print(f'index is: {i}, sum_diff is: {sum_diff}')\n",
        "    acc = 1- (sum_diff / (i+1))\n",
        "    print('Accuracy: in test of deepfool', acc)  # accuracy on perturbed images\n",
        "\n",
        "    # r, loop_i, label_orig, label_pert, pert_image = deepfool(img, net,max_iter=50)\n",
        "\n",
        "    pert_image_numpy = pert_image.detach().squeeze().numpy()\n",
        "    print(pert_image_numpy.shape)\n",
        "    plt.figure()\n",
        "    plt.imshow(pert_image_numpy.transpose(1, 2, 0))\n",
        "    plt.title(label_pert)\n",
        "    #plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(image.cpu().detach().squeeze().numpy().transpose(1, 2, 0))\n",
        "    plt.title(label.item())\n",
        "    #plt.show()\n",
        "\n",
        "    r_new = r.squeeze()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(r_new.transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def adversarial_train(model, epochs, learning_rate, loss_function, train_loader):\n",
        "    loss_arr = []\n",
        "    avg_train_loss_arr= []\n",
        "    train_acc_arr= []\n",
        "    # # Early stopping  parameters\n",
        "    # last_loss = 100  # initializing max loss as high unreachable value\n",
        "    # trigger_times = 0\n",
        "    PATH = './checkpoint'\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00012)\n",
        "    total, correct = 0.0, 0.0\n",
        "    total_adv, correct_adv = 0.0, 0.0\n",
        "    step = 0\n",
        "    # breakstep = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # defining we're training so can use dropout, batch norm\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            if device == torch.device('cuda'):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward and backward propagation\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "            total += train_loader.batch_size\n",
        "\n",
        "            # Adversarial Training\n",
        "            adv_images = batched_deepfool(model, inputs)\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward and backward propagation\n",
        "            #pert_image = pert_image.clone().detach().squeeze(1).to(device)\n",
        "            outputs = model(adv_images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct_adv += torch.sum(predictions == labels).type(torch.float32)\n",
        "            total_adv += train_loader.batch_size\n",
        "\n",
        "            step += 1\n",
        "            if total % 500 == 0:\n",
        "                acc = float(correct) / total\n",
        "                print('[%s] Clean Training accuracy: %.2f%%' % (step, acc * 100))\n",
        "                total = 0\n",
        "                correct = 0\n",
        "                accAdv = float(correct_adv) / total_adv\n",
        "                print('[%s] Adv Training accuracy: %.2f%%' % (step, accAdv * 100))\n",
        "                total_adv = 0\n",
        "                correct_adv = 0\n",
        "    plot_graph(loss_arr, \"generic network training loss\")\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    global device\n",
        "    device = torch.device('cpu')\n",
        "    # check if cuda is available\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    if train_on_gpu:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"CUDA available. Training on GPU\")\n",
        "    else:\n",
        "        print(\"CUDA is not available. Training on CPU\")\n",
        "\n",
        "    train_loader, val_loader, test_loader, test_for_adv, train_for_adv = load_dataset()\n",
        "    batch_size = 64\n",
        "    max_epochs = 80  # number of steps between evaluations\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = CNN_model().to(device)  # with dropout, batch, without FC layers\n",
        "    summary(model, input_size=(3, 32, 32))\n",
        "    print(model)\n",
        "\n",
        "    # model = train_data(model, 100, 0.0001, loss_fn, train_loader, val_loader)\n",
        "    # torch.save(model.state_dict(), PATH)\n",
        "    PATH = './DeepFool/model.pth'\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device(device)))\n",
        "    #model.load_state_dict(torch.load(PATH))\n",
        "    #test_data(model, test_loader)\n",
        "    print('\\n\\nFinished Training model\\n\\n')\n",
        "\n",
        "    adversarial_train(model, 100, 0.0001, loss_fn, train_loader)\n",
        "    #calling_deepfool(model, test_for_adv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmu0PDC0zBqd",
        "outputId": "39a8306d-a834-4b87-cb59-0495c51b2c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available. Training on GPU\n",
            "Files already downloaded and verified\n",
            " trainset: Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               RandomCrop(size=(32, 32), padding=4)\n",
            "               RandomHorizontalFlip(p=0.5)\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
            "           )\n",
            "Files already downloaded and verified\n",
            "train set len 40000\n",
            "validation set len 10000\n",
            "test set len 10000\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          18,496\n",
            "              ReLU-5           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
            "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]         147,584\n",
            "             ReLU-11          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-12            [-1, 128, 8, 8]               0\n",
            "        Dropout2d-13            [-1, 128, 8, 8]               0\n",
            "           Conv2d-14            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-15            [-1, 256, 8, 8]             512\n",
            "             ReLU-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-18            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-19            [-1, 256, 4, 4]               0\n",
            "          Dropout-20            [-1, 256, 4, 4]               0\n",
            "           Conv2d-21            [-1, 512, 1, 1]       2,097,664\n",
            "             ReLU-22            [-1, 512, 1, 1]               0\n",
            "           Conv2d-23            [-1, 256, 1, 1]         131,328\n",
            "             ReLU-24            [-1, 256, 1, 1]               0\n",
            "          Dropout-25            [-1, 256, 1, 1]               0\n",
            "           Conv2d-26             [-1, 64, 1, 1]          16,448\n",
            "             ReLU-27             [-1, 64, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             650\n",
            "================================================================\n",
            "Total params: 3,373,002\n",
            "Trainable params: 3,373,002\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.95\n",
            "Params size (MB): 12.87\n",
            "Estimated Total Size (MB): 16.83\n",
            "----------------------------------------------------------------\n",
            "CNN_model(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05, inplace=False)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.1, inplace=False)\n",
            "    (6): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Finished Training model\n",
            "\n",
            "\n",
            "[125] Clean Training accuracy: 93.05%\n",
            "[125] Adv Training accuracy: 4.49%\n",
            "[250] Clean Training accuracy: 90.70%\n",
            "[250] Adv Training accuracy: 6.42%\n",
            "[375] Clean Training accuracy: 89.61%\n",
            "[375] Adv Training accuracy: 6.98%\n",
            "[500] Clean Training accuracy: 88.49%\n",
            "[500] Adv Training accuracy: 7.50%\n",
            "[625] Clean Training accuracy: 87.88%\n",
            "[625] Adv Training accuracy: 8.22%\n"
          ]
        }
      ]
    }
  ]
}